# ██╗    ██╗███████╗██████╗     ███████╗ ██████╗██████╗  █████╗ ██████╗ ██╗███╗   ██╗ ██████╗ 
# ██║    ██║██╔════╝██╔══██╗    ██╔════╝██╔════╝██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║██╔════╝ 
# ██║ █╗ ██║█████╗  ██████╔╝    ███████╗██║     ██████╔╝███████║██████╔╝██║██╔██╗ ██║██║  ███╗
# ██║███╗██║██╔══╝  ██╔══██╗    ╚════██║██║     ██╔══██╗██╔══██║██╔═══╝ ██║██║╚██╗██║██║   ██║
# ╚███╔███╔╝███████╗██████╔╝    ███████║╚██████╗██║  ██║██║  ██║██║     ██║██║ ╚████║╚██████╔╝
# ╚══╝╚══╝ ╚══════╝╚═════╝     ╚══════╝ ╚═════╝╚═╝  ╚═╝╚═╝  ╚═╝╚═╝     ╚═╝╚═╝  ╚═══╝ ╚═════╝ 


student_name <- "maia arson crimew"


# This week, you're going to scrape a website!
# However, I don't want you to go crazy just yet, so I set up a fake website for 
# you to practice on. You can find it at:

# https://multilingual-computational-methods-2023.github.io/

# These articles and pictures are all AI generated, so don't worry if they don't make sense.

# 1. Find the robots.txt file, and find out what is and is not allowed.

# 2. Find the sitemap, and download it.

# 3. Read the xml of the sitemap, and turn it into a useful data frame.

# 4. Make a file name column for each of the files you want to scrape.

# 5. Save this dataframe, in whatever format you like.

# 6. In a for-loop, scrape every article on the website.
